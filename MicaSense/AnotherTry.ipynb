{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Dodekaphonia/SequoiaStacking/blob/master/parrotStacking.py\n",
    "#https://www.tjscientific.com/2017/06/08/stitching-parrot-sequoia-individual-spectral-band-images-into-a-single-false-color-rgb-composite-image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images(in_fldr, out_fldr, moving, fixed):\n",
    "    MIN_MATCH_COUNT = 10\n",
    "\n",
    "    moving_im = cv2.imread(moving, 0)  # image to be distorted\n",
    "    fixed_im = cv2.imread(fixed, 0)  # image to be matched\n",
    "\n",
    "    # Initiate SIFT detector\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(moving_im, None)\n",
    "    kp2, des2 = sift.detectAndCompute(fixed_im, None)\n",
    "\n",
    "    # use FLANN method to match keypoints. Brute force matches not appreciably better\n",
    "    # and added processing time is significant.\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # store all the good matches following Lowe's ratio test.\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good) > MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        h, w = moving_im.shape  # shape of input images, needs to remain the same for output\n",
    "\n",
    "        outimg = cv2.warpPerspective(moving_im, M, (w, h))\n",
    "\n",
    "        return outimg\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Not enough matches are found for moving image\")\n",
    "        matchesMask = None\n",
    "\n",
    "ch1 = int(input('Which band do you want for channel 1 on output image? Green(1), Red(2), Red Edge(3) or NIR(4)'))\n",
    "ch2 = int(input('Which band do you want for channel 2 on output image? Green(1), Red(2), Red Edge(3) or NIR(4)'))\n",
    "ch3 = int(input('Which band do you want for channel 3 on output image? Green(1), Red(2), Red Edge(3) or NIR(4)'))\n",
    "channel_order = [ch1,ch2,ch3]\n",
    "\n",
    "\n",
    "output_folder = str(input('Enter path to output folder: '))\n",
    "input_folder = str(input('Enter path to input folder: '))\n",
    "\n",
    "image_list = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder,f))]\n",
    "image_tups = zip(*[image_list[i::4] for i in range(4)])\n",
    "\n",
    "# set the fixed image to minimize amount of translation that needs to occur\n",
    "if 1 in channel_order and 2 in channel_order and 3 in channel_order:\n",
    "    fixed_image = 1\n",
    "    moving_im1 = 0\n",
    "    moving_im2 = 2\n",
    "elif 2 in channel_order and 3 in channel_order and 4 in channel_order:\n",
    "    fixed_image = 2\n",
    "    moving_im1 = 1\n",
    "    moving_im2 = 3\n",
    "elif 1 in channel_order and 3 in channel_order and 4 in channel_order:\n",
    "    fixed_image = 2\n",
    "    moving_im1 = 0\n",
    "    moving_im2 = 3\n",
    "elif 1 in channel_order and 2 in channel_order and 4 in channel_order:\n",
    "    fixed_image = 1\n",
    "    moving_im1 = 0\n",
    "    moving_im2 = 3\n",
    "\n",
    "# iterate through each set of 4 images\n",
    "for tup in image_tups:\n",
    "    band1 = align_images(input_folder, output_folder, os.path.join(input_folder, tup[moving_im1]),\n",
    "                         os.path.join(input_folder, tup[fixed_image]))\n",
    "    band2 = align_images(input_folder, output_folder, os.path.join(input_folder, tup[moving_im2]),\n",
    "                         os.path.join(input_folder, tup[fixed_image]))\n",
    "    band3 = cv2.imread(os.path.join(input_folder, tup[fixed_image]), 0)\n",
    "\n",
    "    merged = cv2.merge((band1, band2, band3))\n",
    "\n",
    "    cv2.imwrite(os.path.join(output_folder, tup[fixed_image][-30:-4]) + '_merged.jpg', merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = '/mnt/c/Data/BSU/Photogrammetry/MicaSense_Plants/0033SET/000/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = cv2.imread(wdir +'IMG_0001_1.tif')\n",
    "warp_image = cv2.imread(wdir +'IMG_0001_2.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_detector = cv2.ORB_create(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, des1 = orb_detector.detectAndCompute(base_image, None) \n",
    "kp2, des2 = orb_detector.detectAndCompute(warp_image, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flann = cv2.FlannBasedMatcher(index_params, search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher.match(des1, des2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'cv2.DMatch' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7ef9c0632d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# store all the good matches following Lowe's ratio test.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'cv2.DMatch' object is not iterable"
     ]
    }
   ],
   "source": [
    "# store all the good matches following Lowe's ratio test.\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# store all the good matches following Lowe's ratio test.\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 0, 'trees': 5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checks': 50}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flann = cv2.FlannBasedMatcher(index_params, search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlannBasedMatcher 0x7ff948033510>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8af96f040b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdes1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "des1.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(des1.type()!=CV_32F) {\n",
    "    descriptors_1.convertTo(descriptors_1, CV_32F);\n",
    "}\n",
    "\n",
    "if(des2.type()!=CV_32F) {\n",
    "    descriptors_2.convertTo(descriptors_2, CV_32F);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) /io/opencv/modules/flann/src/miniflann.cpp:315: error: (-210:Unsupported format or combination of formats) in function 'buildIndex_'\n> type=0\n> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-408a0d6e162f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknnMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/flann/src/miniflann.cpp:315: error: (-210:Unsupported format or combination of formats) in function 'buildIndex_'\n> type=0\n> "
     ]
    }
   ],
   "source": [
    "matches = flann.knnMatch(des1, des2, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
